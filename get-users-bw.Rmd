DASH Users Bandwidth
========================================================

For using this script, first it is needed to download the dash dataset at [http://media.polito.it/mmsys14-dataset/neubot-dash-dataset-mmsys14.readme.md](http://media.polito.it/mmsys14-dataset/neubot-dash-dataset-mmsys14.readme.md).
After use the python script convert-json-to-csv.py to convert the dataset to csv.
This script will use the csv file as an input and will produce a .dat file with users and their bandwidth distribution for each of the chosen dash rates.

The result will be a [user x rate] matrix. Each element in the matrix is a "userU_rateR" value that represents the fraction of the time (i.e. a value between 0 and 1) which user "U" has a bandwidth throughput smaller than rate "R". An example with 2 users and 4 rates will be:


|        | 1000kbps | 2000kbps | 3000kbps | 4000kbps |
|:------:|---------:|---------:|---------:|---------:|
| user 1 |        0	|      0.4 |      0.7 |      0.9 |
| user 2 |      0.2	|      0.6 |        1 |        1 |

  
The result file is a plain text .dat [user x rate] matrix, using the first example the final file it will be something like:
[[0, 0.4, 0.7, 0.9],[0.2, 0.6, 1, 1]]

The selection of rates range is:  from 150 kbps up to 12600 kbps with a step of 50 kbps. In total 250 points.

For the users we will select a subset depend on the filtering criterion:
  * average criterion
  * 75th percentile criterion
  
Which means two different output files, accordingly to the users subset, will be produce.

```{r initialization_settings}
# Settings
#rates_kbps = seq(150,12600,by=50) # dash bitrates 
# rates_kbps = c(seq(100,3000,by=100),seq(3500,9000,by=500)) # new dash bitrates
#rates_kbps = c(seq(100,3000,by=100),seq(3250,9000,by=250)) # new dash bitrates
max_users = 500                   # maximum number of users returned
                                  # script will take the max_users that have most samples and meet the criteria
min_samples = 100                 # minimum ammount of samples a user should have, less than that user will be discarted
criteria_bw_kbps = 8000           # limit of the criterias

bps_to_kbps = 1024
bps_to_mbps = 1048576

number_of_samples = 100

path = '/Users/karine/phd/data/dash-users/neubot-parsed/'

rates_kbps = as.vector(read.csv(paste(path,'scenarios_CDF/rates_sc_neutral_dash_users_75perc_bias_percentileDispAssign.txt',sep=''))$rates)

# Libraries
library('plyr')
library('foreach') 
library('ggplot2') 
options(scipen=100)
parallel = TRUE

```


```{r load_files_calc_statistics}

# List of users with problems, i.e. impossible bw capacity
black_list_users = c('0d929236-fc4e-4319-ab49-e08fbcc4a744') 
  

f = paste(path,'201311-201401.csv',sep='')
if(file.exists(f)){
  da = read.csv(f)
}else{
  # Load the csv input files
  files = c('201311.csv','201312.csv','201401.csv')
  d = data.frame()
  for(file in files){
    ft = paste(path,file,sep='')
    t = read.csv(ft)
    d = rbind(d,t)
  }
  
  # Remove initial points (adaptation of dash)
  da = d[d$client_iteration > 4,]
  write.csv(da, f, quote=FALSE, row.names=FALSE)
  d = NULL
}

# Calculate the bandwidth throughput based on bytes received and time elapsed
da$bw = da$client_received/da$client_elapsed # bytes per second
da$bw_bits = da$bw*8 # bits per second
da$bw_mpbs = da$bw_bits/bps_to_mbps

# Create new user id based on ip + uuid
da$ipuserid = paste(da$client_real_address,'-',da$client_uuid,sep='')

f = paste(path,'user-statistics.csv',sep='')
if(file.exists(f)){
  du = read.csv(f)
}else{
 # Calculate the number of samples, average and 75% of all users
  (z <- Sys.time())
  du = ddply(da,~ipuserid, summarize, .parallel=parallel,
    samples=length(ipuserid),
  	min=min(bw_bits), 
  	mean=mean(bw_bits), 	
  	median=median(bw_bits),
  	max=max(bw_bits),
  	stdv=sd(bw_bits),
    percentile75=quantile(bw_bits,0.75))
  (z <- Sys.time())
  write.csv(du, f, quote=FALSE, row.names=FALSE)
}

```

Verification of some users:

```{r user_example}

# user_index = 1
# d1u = da[da$ipuserid == as.character(du$ipuserid[user_index]),]
# plot(qplot(d1u$bw_mpbs, stat = "ecdf", geom = "step", xlab="bandwidth (Mbps)",ylab="samples"))

```

Select the subset accordingly to the criterias:

```{r select_subsets}
f_avg = paste(path,'dash_users_avg.dat',sep='')
f_perc = paste(path,'dash_users_75perc.dat',sep='')
f_sample_avg = paste(path,'dash_users_avg_samples_',number_of_samples,'.csv',sep='')
f_sample_perc = paste(path,'dash_users_75perc_samples_',number_of_samples,'.csv',sep='')
fab_avg = paste(path,'dash_avgbitrates_avg.dat',sep='')
fab_perc = paste(path,'dash_avgbitrates_75perc.dat',sep='')
fmb_avg = paste(path,'dash_medianbitrates_avg.dat',sep='')
fmb_perc = paste(path,'dash_medianbitrates_75perc.dat',sep='')
f_consec_sample_avg = paste(path,'dash_users_avg_consec_samples_',number_of_samples,'.csv',sep='')
f_consec_sample_perc = paste(path,'dash_users_75perc_consec_samples_',number_of_samples,'.csv',sep='')
f_raw_users_bw_perc = paste(path,'raw_users_bw/dash_users_75perc_',sep='')

if(!file.exists(f_avg) || !file.exists(f_perc) ||
   !file.exists(f_sample_avg) || !file.exists(f_sample_perc) ||
   !file.exists(f_consec_sample_avg) || !file.exists(f_consec_sample_perc) ||
   !file.exists(fmb_avg) || !file.exists(fmb_perc) ||
   !file.exists(f_raw_users_bw_perc) ||
   !file.exists(fab_avg) || !file.exists(fab_perc)){
  criteria_bw_bps = criteria_bw_kbps*bps_to_kbps
  
  # Users that meet average criteria and samples
  du_avg = du[du$mean < criteria_bw_bps & du$samples >= min_samples,  ] # & !(du$client_uuid %in% black_list_users) not needed since the bw criteria will cut this users
  du_avg = du_avg[order(-du_avg$samples),] # order by the number of samples
  # If we have more than needed we take only max_users with more samples
  if(length(du_avg$ipuserid) > max_users){
    du_avg = du_avg[1:max_users,]  
  }
  
  
  # Users that meet percentile criteria and samples
  du_perc = du[du$percentile75 < criteria_bw_bps & du$samples >= min_samples,  ]
  du_perc = du_perc[order(-du_perc$samples),] # order by the number of samples
  # If we have more than needed we take only max_users with more samples
  if(length(du_perc$ipuserid) > max_users){
    du_perc = du_perc[1:max_users,]  
  }
}

```

Create the matrix [user x rates]:

```{r matrix_users_rates}

if(file.exists(f_avg) && file.exists(f_perc)){
  print('Output dat files already exists. Delete manually to recreate.')
}else{
  rates_bps = rates_kbps*bps_to_kbps
  
  mu_avg = matrix(nrow = length(du_avg$ipuserid), ncol = length(rates_bps))
  for(user_index in 1:length(du_avg$ipuserid)){
    t = da[da$ipuserid == as.character(du_avg$ipuserid[user_index]),]
    fnecdf=ecdf(t$bw_bits)
    mu_avg[user_index,] = fnecdf(rates_bps)
  }
    
  mu_perc = matrix(nrow = length(du_perc$ipuserid), ncol = length(rates_bps))
  for(user_index in 1:length(du_perc$ipuserid)){
    t = da[da$ipuserid == as.character(du_perc$ipuserid[user_index]),]
    fnecdf=ecdf(t$bw_bits)
    mu_perc[user_index,] = fnecdf(rates_bps)
  }
}    
```

Output the results:

```{r output_file}
if(!(file.exists(f_avg) && file.exists(f_perc))){
  matrix2dat <- function(m){
    mfinal = apply(m,1,paste,sep="",collapse=',')
    mfinal = paste('[',mfinal,']',sep='',collapse=',')
    mfinal = paste('[',mfinal,']',sep='')
    return(paste(mfinal,sep="",collapse=','))
  }
  
  write(matrix2dat(mu_avg), file=f_avg)
  write(matrix2dat(mu_perc), file=f_perc)
}
```

Create the average bitrate files:

```{r avg_bitrates}

array2dat <- function(m){
  mfinal = paste(m,sep='',collapse=',')
  return(paste('[',mfinal,']',sep='',collapse=','))
}

if(file.exists(fab_avg) && file.exists(fab_perc)){
  print('Output average files already exists. Delete manually to recreate.')
}else{
  write(array2dat(du_avg$mean/bps_to_kbps), file=fab_avg)
  write(array2dat(du_perc$mean/bps_to_kbps), file=fab_perc)
}

```

Select random samples of bitrates of users:

```{r random_samples}

if(file.exists(f_sample_avg) && file.exists(f_sample_perc)){
  print('Output sample files already exists. Delete manually to recreate.')
}else{
  
  da_u_avg_perc = da[da$ipuserid %in% du_avg$ipuserid | da$ipuserid %in% du_perc$ipuserid ,]
  
  (z <- Sys.time())
  users_bw_samples = ddply(da_u_avg_perc,~ipuserid,summarize, .parallel=parallel,
                          samples=sample(bw_bits,number_of_samples))
  (z <- Sys.time())
  
  mu_avg_sample = matrix(nrow = length(du_avg$ipuserid), ncol = number_of_samples)
  for(user_index in 1:length(du_avg$ipuserid)){
    mu_avg_sample[user_index,] = users_bw_samples[users_bw_samples$ipuserid == as.character(du_avg$ipuserid[user_index]),'samples']
  }
    
  mu_perc_sample = matrix(nrow = length(du_perc$ipuserid), ncol = number_of_samples)
  for(user_index in 1:length(du_perc$ipuserid)){
    mu_perc_sample[user_index,] = users_bw_samples[users_bw_samples$ipuserid == as.character(du_perc$ipuserid[user_index]),'samples']
  }
  
  write.table(mu_avg_sample, file=f_sample_avg, row.names=FALSE, col.names=FALSE, sep=",")
  write.table(mu_perc_sample, file=f_sample_perc, row.names=FALSE, col.names=FALSE, sep=",")
  
}    
```

Select consecutive samples of bitrates of users:

```{r consec_samples}

consec_sample <- function(x,n){
  return(x[1:n])
}

if(file.exists(f_consec_sample_avg) && file.exists(f_consec_sample_perc)){
  print('Output sample files already exists. Delete manually to recreate.')
}else{
  
  da_u_avg_perc = da[da$ipuserid %in% du_avg$ipuserid | da$ipuserid %in% du_perc$ipuserid ,]
  
  (z <- Sys.time())
  users_bw_samples = ddply(da_u_avg_perc,~ipuserid,summarize, .parallel=parallel,
                          samples=consec_sample(bw_bits,number_of_samples))
  (z <- Sys.time())
  
  mu_avg_sample = matrix(nrow = length(du_avg$ipuserid), ncol = number_of_samples)
  for(user_index in 1:length(du_avg$ipuserid)){
    mu_avg_sample[user_index,] = users_bw_samples[users_bw_samples$ipuserid == as.character(du_avg$ipuserid[user_index]),'samples']
  }
    
  mu_perc_sample = matrix(nrow = length(du_perc$ipuserid), ncol = number_of_samples)
  for(user_index in 1:length(du_perc$ipuserid)){
    mu_perc_sample[user_index,] = users_bw_samples[users_bw_samples$ipuserid == as.character(du_perc$ipuserid[user_index]),'samples']
  }
  
  write.table(mu_avg_sample, file=f_consec_sample_avg, row.names=FALSE, col.names=FALSE, sep=",")
  write.table(mu_perc_sample, file=f_consec_sample_perc, row.names=FALSE, col.names=FALSE, sep=",")
  
}    
```

Create the median bitrate files:

```{r median_bitrates}

array2dat <- function(m){
  mfinal = paste(m,sep='',collapse=',')
  return(paste('[',mfinal,']',sep='',collapse=','))
}

if(file.exists(fmb_avg) && file.exists(fmb_perc)){
  print('Output average files already exists. Delete manually to recreate.')
}else{
  write(array2dat(du_avg$median/bps_to_kbps), file=fmb_avg)
  write(array2dat(du_perc$median/bps_to_kbps), file=fmb_perc)
}

```

Create raw bandwidth of users:

```{r raw_users_bw}

if(file.exists(f_raw_users_bw_perc)){
  print('Output sample files already exists. Delete manually to recreate.')
}else{
  write('test',f_raw_users_bw_perc)
  
  da_u_avg_perc = da[da$ipuserid %in% du_avg$ipuserid | da$ipuserid %in% du_perc$ipuserid ,]
  da_u_avg_perc$bw_kbps = da_u_avg_perc$bw_bits/bps_to_kbps
  
  for(user_index in 1:length(du_perc$ipuserid)){
    f_raw_one_user_bw_perc = paste(f_raw_users_bw_perc,'_u',user_index,'.csv',sep='')
    write.table(da_u_avg_perc[da_u_avg_perc$ipuserid == as.character(du_perc$ipuserid[user_index]),'bw_kbps'], file=f_raw_one_user_bw_perc, row.names=FALSE, col.names=FALSE, sep=",")
  }
  
}    
```


